<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/blog/2020/11/06/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>深度学习_数学</title>
    <url>/blog/2020/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6/</url>
    <content><![CDATA[<h1 id="一-线代内容"><a href="#一-线代内容" class="headerlink" title="一 线代内容"></a>一 线代内容</h1><h2 id="1-标量、向量、矩阵、张量之间的联系"><a href="#1-标量、向量、矩阵、张量之间的联系" class="headerlink" title="1 标量、向量、矩阵、张量之间的联系"></a>1 标量、向量、矩阵、张量之间的联系</h2><h3 id="1-1-张量"><a href="#1-1-张量" class="headerlink" title="1.1 张量"></a>1.1 张量</h3><p>可以理解为矩阵的升级版，矩阵是二维的，张量可以是无限维。可以说标量就是0阶张量、向量就是1阶张量、矩阵就是2阶张量。</p>
<h3 id="1-2-向量和矩阵的范数归纳"><a href="#1-2-向量和矩阵的范数归纳" class="headerlink" title="1.2  向量和矩阵的范数归纳"></a>1.2  向量和矩阵的范数归纳</h3><h4 id="1-2-1-向量的范数"><a href="#1-2-1-向量的范数" class="headerlink" title="1.2.1 向量的范数"></a>1.2.1 向量的范数</h4><p><strong>向量的L1范数</strong></p>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert</script><p><strong>向量的L2范数</strong></p>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}</script><p>这两个为最常见的范数形式，还有向量的负无穷范数、正无穷范数、p范数。<br><a href="https://blog.csdn.net/a493823882/article/details/80569888">范数简介</a></p>
<h4 id="1-2-2-矩阵的范数"><a href="#1-2-2-矩阵的范数" class="headerlink" title="1.2.2 矩阵的范数"></a>1.2.2 矩阵的范数</h4><p>矩阵的范数定义为</p>
<script type="math/tex; mode=display">
\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}</script><p>当向量取不同范数时, 相应得到了不同的矩阵范数。<br><strong>矩阵的1-范数（列模）</strong><br>矩阵的每一列上的元素绝对值先求和，再从中取个最大的</p>
<script type="math/tex; mode=display">
\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|</script><p>总结就是列和最大。<br><strong>矩阵的2-范数（谱模）</strong><br>矩阵$A^TA$的最大特征值开平方根</p>
<script type="math/tex; mode=display">
\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}</script><p>其中， $\lambda_{max}(A^T A)$ 为 $A^T A​$ 的特征值绝对值的最大值。总结就是$A^T A​$最大特征根开平方根。矩阵还有无穷范数、核范数、L0范数、L1范数、F范数、L21范数。</p>
<h2 id="1-3-判断矩阵为正定"><a href="#1-3-判断矩阵为正定" class="headerlink" title="1.3 判断矩阵为正定"></a>1.3 判断矩阵为正定</h2><p><strong>定义</strong>：设$M$是n阶方阵，如果对任何非零向量z，都有$z^TMz&gt; 0$，其中$z^T$表示$z$的转置，就称$M$为正定矩阵。<br><strong>性质</strong>：<br>（1）正定矩阵的行列式恒为正；<br>（2）实对称矩阵A正定当且仅当A与单位矩阵合同；<br>（3）若A是正定矩阵，则A的逆矩阵也是正定矩阵；<br>（4）两个正定矩阵的和是正定矩阵；<br>（5）正实数与正定矩阵的乘积是正定矩阵。</p>
<h1 id="2-特征值和特征向量"><a href="#2-特征值和特征向量" class="headerlink" title="2 特征值和特征向量"></a>2 特征值和特征向量</h1><p>特征值分解可以得到特征值和特征向量。特征值表示的这个特征有多重要，而特征向量表示这个特征是什么。<br><a href="https://www.zhihu.com/people/zzz-22-60-93/activities">特征向量和特征值的解释</a></p>
<script type="math/tex; mode=display">
A\nu = \lambda \nu</script><p>$\lambda$为特征向量$\vec{v}$对应的特征值。</p>
<h1 id="二-概率论内容"><a href="#二-概率论内容" class="headerlink" title="二 概率论内容"></a>二 概率论内容</h1><h2 id="1-机器学习为什么要使用概率论"><a href="#1-机器学习为什么要使用概率论" class="headerlink" title="1 机器学习为什么要使用概率论"></a>1 机器学习为什么要使用概率论</h2><p>机器学习除了处理不确定的变量，也要处理随机变量。不确定和随机性来自多个方面，概率论来量化不确定性。</p>
<h2 id="2-概率分布"><a href="#2-概率分布" class="headerlink" title="2 概率分布"></a>2 概率分布</h2><h3 id="2-1-正态分布"><a href="#2-1-正态分布" class="headerlink" title="2.1 正态分布"></a>2.1 正态分布</h3><p>从概率论中，我们掌握很多概率分布，如高斯分布、布尔分布、指数分布、拉普拉斯分布等等，但正态分布无疑是最有特殊性的一个。<br>那么我们什么时候会用正太分布呢？在机器学习中，<strong>我们缺乏实数上分布的经验知识，不知道采取何种形式时</strong>，默认选择正态分布总是不会错的。理由如下：</p>
<ol>
<li>中心极限定理告诉我们，很多独立的随机变量近似服从正态分布，现实中很多复杂的系统都可以被建模成正太分布的噪声，即使该系统可以被结构化分解。</li>
<li>正态分布是具有相同方差的所有概率分布，不确定性最大的分布，换句话说正态分布是对模型加入先验知识最少的分布。<h3 id="2-2-正态分布的推广"><a href="#2-2-正态分布的推广" class="headerlink" title="2.2 正态分布的推广"></a>2.2 正态分布的推广</h3>正太分布可以推广到$R^n$空间，此时称为<strong>多维正态分布</strong><script type="math/tex; mode=display">
N(x;\vec\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right)</script><a href="https://www.zhihu.com/question/36339816/answer/67043318">多维正态分布</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>深度学习_机器学习</title>
    <url>/blog/2020/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>参考资料来自<a href="https://github.com/comical233/DeepLearning-500-questions/blob/master/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md">github</a>和《深度学习》此书</p>
<h1 id="1-导言"><a href="#1-导言" class="headerlink" title="1 导言"></a>1 导言</h1><p>机器学习起源于上世纪50年代，1959年在IBM工作的Arthur Samuel设计了一个下棋程序，这个程序具有学习的能力，它可以在不断的对弈中提高自己。由此提出了“机器学习”这个概念。<br>当下研究火热的人工智能，机器学习是其的一个子集。机器学习在很多方面已经有了许多的应用，使用各种算法，如支持向量机，决策树、随机森林等等，一定程度上可以帮助人们完成一些数据预测，自动化，自动决策，最优化等初步替代脑力的任务。</p>
<h1 id="2-基本概念"><a href="#2-基本概念" class="headerlink" title="2 基本概念"></a>2 基本概念</h1><h2 id="2-1-机器学习的本质"><a href="#2-1-机器学习的本质" class="headerlink" title="2.1 机器学习的本质"></a>2.1 机器学习的本质</h2><p>机器学习本质来讲就是一个算法黑箱，有输入有输出。一般来讲，机器学习的任务不是无中生有，让机器去发现事务规律，而是人们将事务规律转换为机器算法，让算法提取中数据所蕴含的规律，这就叫做机器学习。如果输入机器的数据是带有标签的，就称作有监督学习。如果是无标签的，就是无监督学习。<br>那么我所谓的“学习”具体指什么呢？Mithchell（1997）提供了一个简洁的定义：“对于某类任务$T$和性能度量$P$，一个计算机程序被认为可以从经验$E$中学习是指，通过经验$E$改进后，它在任务$T$上由性能度量$P$衡量的性能有所提升。”<br>任务$T$：分类、回归、转录、机器翻译、去噪……<br>度量$P$：准确度、错误率……</p>
<h2 id="2-2-机器学习的分类"><a href="#2-2-机器学习的分类" class="headerlink" title="2.2 机器学习的分类"></a>2.2 机器学习的分类</h2><p>主要分为四类</p>
<h3 id="2-2-1-监督学习"><a href="#2-2-1-监督学习" class="headerlink" title="2.2.1 监督学习"></a>2.2.1 监督学习</h3><p>特点：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。<br>常见应用场景：监督式学习的常见应用场景如分类问题和回归问题。<br>常用算法举例：支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。</p>
<h3 id="2-2-2-非监督学习"><a href="#2-2-2-非监督学习" class="headerlink" title="2.2.2 非监督学习"></a>2.2.2 非监督学习</h3><p>定义：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构。<br>常见应用场景：常见的应用场景包括关联规则的学习以及聚类等。<br>算法举例：常见算法包括Apriori算法以及k-Means算法。<br><a href="https://blog.csdn.net/huihuisd/article/details/86489810">Apriori算法</a></p>
<h3 id="2-2-3半监督式学习"><a href="#2-2-3半监督式学习" class="headerlink" title="2.2.3半监督式学习"></a>2.2.3半监督式学习</h3><p>特点：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。<br>​常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。<br>​算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。</p>
<h3 id="2-2-4-弱监督学习"><a href="#2-2-4-弱监督学习" class="headerlink" title="2.2.4 弱监督学习"></a>2.2.4 弱监督学习</h3><p>特点：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。<br>​算法举例：举例，给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线，这就是已知弱标签学习强标签的问题。</p>
<h2 id="2-3-分类算法"><a href="#2-3-分类算法" class="headerlink" title="2.3 分类算法"></a>2.3 分类算法</h2><p>分类算法和回归算法是不同的。分类模型是认为模型的输出是离散的，例如大自然的生物被划分为不同的种类，是离散的。回归模型的输出是连续的，例如人的身高变化过程是一个连续过程，而不是离散的。<br>​因此，在实际建模过程时，采用分类模型还是回归模型，取决于你对任务（真实世界）的分析和理解。</p>
<h3 id="2-3-1-分类算法的优缺点"><a href="#2-3-1-分类算法的优缺点" class="headerlink" title="2.3.1 分类算法的优缺点"></a>2.3.1 分类算法的优缺点</h3><p><img src="https://img-blog.csdnimg.cn/20200228153522669.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMTE1MTIy,size_16,color_FFFFFF,t_70" alt="常见算法小结"></p>
<h3 id="2-3-2-评价方法"><a href="#2-3-2-评价方法" class="headerlink" title="2.3.2 评价方法"></a>2.3.2 评价方法</h3><p>ROC曲线和PR曲线<a href="https://www.jianshu.com/p/ac46cb7e6f87">详解</a></p>
<p><img src="https://img-blog.csdnimg.cn/20200228154642414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMTE1MTIy,size_16,color_FFFFFF,t_70#pic_center =400x300" alt="示例"><br>如果一个学习器的P-R曲线被另一个学习器的P-R曲线完全包住，则可断言后者的性能优于前者，例如上面的A和B优于学习器C。但是A和B的性能无法直接判断，我们可以根据曲线下方的面积大小来进行比较，但更常用的是平衡点或者是F1值。平衡点（BEP）是P=R时的取值，如果这个值较大，则说明学习器的性能较好。而F1  =  2 <em> P </em> R ／( P + R )，同样，F1值越大，我们可以认为该学习器的性能较好。</p>
<h1 id="3-梯度下降"><a href="#3-梯度下降" class="headerlink" title="3 梯度下降"></a>3 梯度下降</h1><p>梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。（摘自百度百科）</p>
<p><img src="https://img-blog.csdnimg.cn/20200228160542116.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMTE1MTIy,size_16,color_FFFFFF,t_70#pic_center =400x300" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20200228160509553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMTE1MTIy,size_16,color_FFFFFF,t_70#pic_center =400x300" alt="在这里插入图片描述"><br>梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：<br>（1）梯度下降是迭代法的一种，可以用于求解最小二乘问题。<br>（2）在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。<br>（3）在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。<br>（4）如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。<br>（5）在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</p>
<p>当然梯度下降也存在缺点：<br>（1）靠近极小值时收敛速度减慢。<br>（2）直线搜索时可能会产生一些问题。<br>（3）可能会“之字形”地下降。</p>
]]></content>
  </entry>
</search>
